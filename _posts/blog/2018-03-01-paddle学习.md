
---
layout: post
title: paddle学习
categories: 机器学习
description: 
keywords: paddle
---



# 1. paddle的输入、输出

+ 首先，引入相关包

```python
import paddle.v2 as paddle
from paddle.trainer_config_helpers import *
```

## 1.1. 各种输入

+ sparse non weight

+ 对于one-hot和multi-hot的输入，都是这个类型。这种类型后面接入embedding层，然后再接一个pooling层。

```python
data1 = paddle.layer.data("slot1", paddle.data_type.integer_value_sequence(2000000))
```

+ sparse weight

```python
data2 = paddle.layer.data("slot2", paddle.data_type.sparse_float_vector(2000000))
```

+ dense

```python
data3=paddle.layer.data("slot3", paddle.data_type.dense_vector(64))
```

## 1.2. 输出

```python
lbl = paddle.layer.data("label", paddle.data_type.integer_value(2))
```



## 1.3. 损失

```python
cost=paddle.layer.classification_cost(input=output, label=lbl)
```

+ 如果做Multi-Task Learning，用类似下面的代码：

```python
loss1 = paddle.layer.classification_cost(input = out1, label = lbl, coeff = 0.3)
loss2 = paddle.layer.classification_cost(input = out2, label = lbl, coeff = 0.7)
```



---

#2. paddle的layer



+ embedding层


```python
emb1=paddle.layer.embedding(name="emb1", 
                            input=data1,   #data1 is sparse non weight
                            size=64,
                            param_attr=paddle.attr.Param(initial_std=std),
                            layer_attr=paddle.attr.ExtraAttr(error_clipping_threadshold=10))
```

+  一般地，embedding层不能直接接入fc层，如果需要接入，需要中间用pooling层过渡一下。所以不管是one-hot、multi-hot的稀疏输入，都得接一个pooling层。

```python
emb1_pooling=paddle.layer.pooling(name="emb1_pooling",
							    input=emb1,
							    pooling_type=paddle.pooling.Avg)
```

+ concat层

```python
concat_layer=paddle.layer.concat(name="concat_layer",
							   input=[emb1_pooling, data3])
```



+ dropout层

```python
dropout_layer=paddle.layer.dropout(input=concat_layer, dropout_rate=0.2)
```


+ fc层


```python
hd1_layer=paddle.fc.layer.fc(name="hd1",
					       input=[dropout_layer],
                             size=1024,
                             act=paddle.activation.Relu())
```

+ BN层

```python
bn_layer = paddle.layer.batch_norm(name='bn1', 
                                   input = hd1_layer, 
                                   act = paddle.activation.Relu())
```





+ Attention网络的实现

```python
# input layers
data1 = paddle.layer.data("slot1", paddle.data_type.integer_value_sequence(2000000))
data2 = paddle.layer.data("slot2", paddle.data_type.integer_value_sequence(2000000))

# embedding layer
emb1 = paddle.layer.embedding(name = "emb1",   #multi-hot
                                input = data1,
                                size = 64,
                                param_attr = paddle.attr.Param(initial_std=init_std(64)),
                              layer_attr = paddle.attr.ExtraAttr(error_clipping_threshold=10.0))

emb1 = paddle.layer.embedding(name = "emb2",   #one-hot
                                input = data2,
                                size = 64,
                                param_attr = paddle.attr.Param(initial_std = init_std(64)),
                              layer_attr = paddle.attr.ExtraAttr(error_clipping_threshold=10.0))

emb2_pool = paddle.layer.pooling(name="emb2_pool",
    							input=emb2,      
                                   pooling_type=paddle.pooling.Avg())

print "begin attention"
expanded = paddle.layer.expand(input=emb2_pool, expand_as=emb1)
m = paddle.layer.dot_prod(input1=expanded, input2=emb1)
attention_weight = paddle.layer.fc(name = "attention_layer",
                                   input = m,
                                   size = 1,
                                   act = paddle.activation.SequenceSoftmax(),
                                   bias_attr = False)

scaled = paddle.layer.scaling(weight=attention_weight, input=emb1)

attended = paddle.layer.pooling(input=scaled, pooling_type=paddle.pooling.Sum())
print "after attention network"
```





---

# 3. paddle的模型文件的操作

+ 将paddle保存的二进制参数还原成明文

```python
def read_parameter(fname, width):
    s = open(fname).read()
    # 跳过16 位 头信息
    vec = np.fromstring(s[16:], dtype=np.float32)
    # 需要指定 width 还原回原始矩阵的形状，width 是配置中layer的size
    np.savetxt(fname + ".csv", vec.reshape(width, -1), fmt="%.6f", delimiter=",")
```

​        https://github.com/PaddlePaddle/Paddle/issues/2812

+ 设置某一层参数值

```python
s = open("_fc1_layer.w0").read()
vec = np.fromstring(s[16:], dtype=np.float32)
a_shape=dnn_parameters.get_shape("_fc1_layer.w0")
dnn_parameters.set("_fc1_layer.w0", vec.reshape(a_shape))
```



+ 获取某一层参数矩阵

```python
s = open("_emb1_1.w0").read()
vec = np.fromstring(s[16:], dtype=np.float32)
emb_matrix = vec.reshape([20000,64])
```

+ 获取某一层bias参数

```python
s = open( "_fc1_layer.wbias").read()
vec = np.fromstring(s[16:], dtype=np.float32)
emb_bias = vec.reshape(-1)
```



+ 参数打包

+ 压缩的时候不要带入当前目录，进入参数的当前目录下，执行

```shell
tar czvf dnn_params.tar.gz *
```



# 4. paddle的训练和预测

## 4.1 MPI集群训练

+ kill线上训练任务

```python
cd 提交任务目录
smart/bin/qdel -S nmg-hpc-hlan-smart-master.dmop.baidu.com -O 8949 app-user-20180418132808-254
```

+ MPI集群使用参考：http://agroup.baidu.com/recommendation/md/article/644790



## 4.2 预测

```python
def dnn_infer(param_file_name):
    print("Begin to predict...")
    #parameters = paddle.parameters.Parameters.from_tar(gzip.open(param_file_name))
    parameters = paddle.parameters.create(cost)
    feeding = ['slot1', 'slot2', 'slot3', 'slot4','slot5', 'slot6', 'slot7', 'slot8',
               'slot9', 'slot10', 'slot11', 'slot12', 'slot13', 'slot14', 'slot15',
               'slot16', 'slot17', 'slot18', 'slot19', 'slot20', 'slot21', 'slot22',
               'slot23', 'slot24', 'slot25', 'slot26', 'slot27', 'slot28', 'slot29',
               'slot30', 'slot31', 'slot32', 'slot33', 'slot34','slot35','slot36']

    infer_data = []
    online_dnn_score_list = []
    for item in cluster_data_reader(cluster_test_dir, node_id)():
        infer_data.append(item[0:-1])
        online_dnn_score_list.append(item[-1])

    predictions = paddle.infer(output_layer=output,
                               parameters=parameters,
                               input=infer_data,
                               field=['value'],
                               feeding=feeding)
    print "online,   offline"
    #print predictions
    for i in range(0, len(predictions)):
        online_score = online_dnn_score_list[i]
        offline_score = predictions[i][1]
        print online_score, offline_score
    #end-for
```



## 4.3 打印中间层

```python
def dnn_infer_layer(param_file_name):
	print("Begin to predict..middle layer")
	parameters = paddle.parameters.Parameters.from_tar(gzip.open(param_file_name))
	#parameters = paddle.parameters.create(cost)
	feeding = ['slot1', 'slot2', 'slot3', 'slot4','slot5', 'slot6', 'slot7', 'slot8',
			   'slot9', 'slot10', 'slot11', 'slot12', 'slot13', 'slot14', 'slot15',
   	            'slot16', 'slot17', 'slot18', 'slot19', 'slot20', 'slot21', 'slot22',
                'slot23', 'slot24', 'slot25', 'slot26', 'slot27', 'slot28', 'slot29',
                'slot30', 'slot31', 'slot32', 'slot33', 'slot34','slot35','slot36']
	infer_data = []
	online_dnn_score_list = []
	for item in cluster_data_reader(cluster_test_dir, node_id)():
		infer_data.append(item[0:-1])
		online_dnn_score_list.append(item[-1])
		numpy.set_printoptions(threshold=numpy.nan)
		res = [dict() for i in range(0, len(infer_data))]
		middle_layers = [ (1,emb1_1_pooling), (2, emb1_2_pooling) , (3,emb1_3_pooling),
                          (4,emb1_4_pooling), (5, data5),           (6, data6),
                          (7,emb1_7_pooling), (8, emb1_8_pooling),  (9, data9),
                          (10,data10),        (11, data11),         (12,emb1_12),
                          (13,data13),        (14, data14),         (15,data15),
                          (16,data16),        (17, data17),         (18,emb1_18_pooling),
                          (19,data19),        (20, emb1_20),        (21,emb1_21),
                          (22,emb1_22),       (23,data23),          (24,emb1_24),
                          (25,emb1_25),       (26,data26),          (27,data27),
                          (28,data28),        (29,emb1_29),         (30,emb1_30),
                          (31,emb1_31),       (32,emb1_32),         (33,emb1_33_pooling),
                          (34,emb1_34_pooling), (35,emb1_35_pooling), (36,emb1_36_pooling)]
        for item in middle_layers:
           slot=item[0]
           emb=item[1]
           print_layer = paddle.infer(output_layer= emb,
                                      parameters=parameters,
                                      input=infer_data,
                                      field=['value'],
                                      feeding=feeding)
           print len(print_layer)
           #print predictions
           for i in range(0, len(print_layer)):
                res[i][slot] = print_layer[i]
        #end-for
        for i in range(0,len(res)):
            print "sample"+str(i+1), ":"
            temp=sorted(res[i].items(), key=lambda a:a[0])
            for a,b in temp:
                print "slot_"+str(a),"=",
                for x in b:
                    print x,
               print ""
           print "---------------------------------------------"
        #end-for
    #end-for
```







---

# 4. paddle的安装指南



## 4.1 安装paddle单机版，第一坑

+ 如果是厂内机器libstdc++等库版本比较老，可以下载下面的python27-gcc482环境。我踩的第一个坑就是老机器的坑，然后安装过程各种错误。其实，只要按照下面的执行步骤来做就好啦。

+  http://wiki.baidu.com/pages/viewpage.action?pageId=327596892

```python
wget ftp://yq01-idl-gpu-offline14.yq01.baidu.com/tmp/python27-gcc482.tar.gz
tar -zxvf python27-gcc482.tar.gz
cd python27-gcc482
bin/python bin/pip install -i http://pip.baidu.com/pypi/simple --trusted-host pip.baidu.com  --upgrade pip
bin/python bin/pip install -i http://pip.baidu.com/pypi/simple --trusted-host pip.baidu.com paddlepaddle
```



##4.2 paddle的C-API      

+ 参考
 http://wiki.baidu.com/pages/viewpage.action?pageId=358079332



+ paddle的machine初始化以及代码

```c++
#include <paddle/capi.h>
paddle_gradient_machine machine;

void *buf = read_config("trainer_config.bin",&size); //移步上面的参考链接
paddle_gradient_machine_create_for_inference(&machine, buf, (int)size);  // Create machine.
paddle_gradient_machine_load_parameter_from_disk(machine, "params_dir");  // Loading params

paddle_gradient_machine machine_x; //有可能有多个machine
paddle_gradient_machine_create_shared_param(machine, buf, size, &machine_x);
free(buf);

//use macine
{
	.....   
}

paddle_gradient_machine_destroy(machine);
paddle_gradient_machine_destroy(machine_x);
```



+ 给paddle的machine输入数据

```c++
paddle_arguments in_args = paddle_argument_create_none();
paddle_arguments_resize(in_args, cnt_slots);

for (int slot_index : slot_index_list) { //遍历slot
	paddle_matrix mat = paddle_matrix_create(cnt_samples, slot_dim, false);
	paddle_real *array;
	for (int i = 0; i < cnt_samples; ++i) { //遍历样本
    	paddle_matrix_get_row(mat, i, &array);
    	copy(x.begin(), x.end(), array);
	}

	paddle_arguments_set_value(in_args, slot_index, mat);
	paddle_matrix_destroy(mat);
}
```



+ 用paddle进行预测打分

```c++
paddle_arguments out_args = paddle_argument_create_none();
paddle_gradient_machine_forword(machine, in_args, our_args, false);
paddle_real *array;
paddle_matrix Y_hat = paddle_matrix_create_none();
paddle_arguments_get_value(out_args, 0, y_hat);
for (int i = 0; i < cnt_samples; ++i) {
    paddle_matrix_get_row(Y_hat, i, &array);
    float score = array[1];
    cout << score<<std::endl;
}
paddle_matrix_destroy(Y_hat);
```



